{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb982d0-d6b7-43bb-a9f5-8a0d805c2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from html import unescape\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import nltk\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7298cef4-327b-45cf-a900-744ecf89e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d8522-bdc8-4e95-8bcb-347e68f1d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "questions_df = pd.read_csv('Questions.csv', encoding='latin1')\n",
    "tags_df = pd.read_csv('Tags.csv', encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53644c97-3831-488d-89b9-69cfe09cbce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge questions and tags\n",
    "questions_tags_df = pd.merge(questions_df, tags_df, how='inner', on='Id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236b377-5663-4ed9-b32e-5a17f5216bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and body into one text field\n",
    "questions_tags_df['Text'] = questions_tags_df['Title'] + \" \" + questions_tags_df['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6630f1d7-4576-41d1-9391-5d08db4c0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean HTML tags and entities\n",
    "def clean_html(text):\n",
    "    return re.sub(r'<.*?>', '', text)  # Remove HTML tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc14a055-9ce1-4b47-b884-5b42be208097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clean_html using vectorized operation\n",
    "questions_tags_df['Text'] = questions_tags_df['Text'].apply(clean_html).apply(unescape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fb567-f123-4601-8d18-7adf36cbccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process text\n",
    "def process_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenize, remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in word_tokenize(text.lower()) if token.isalpha() and token not in stop_words]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d232ab-71b5-4e80-bd5f-e6d9095e25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize the text processing\n",
    "def parallelize_dataframe(df, func, num_cores=4):\n",
    "    df_split = np.array_split(df, num_cores)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534b661-49d7-4c73-b100-e74a5633960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply process_text in parallel\n",
    "questions_tags_df['Cleaned_Text'] = parallelize_dataframe(questions_tags_df['Text'], process_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c08a50-a35d-4976-b21f-b833b0ec43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in Tags (if any)\n",
    "questions_tags_df.dropna(subset=['Tags'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d426f-c2cf-45a5-880a-8615726c2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Tags column to list of strings\n",
    "questions_tags_df['Tags'] = questions_tags_df['Tags'].apply(lambda x: x.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02400e8-8770-44c4-8d85-c2b36f6177d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiLabelBinarizer for Tags\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(questions_tags_df['Tags'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f55e1-d1d5-4c5c-b9eb-1c85b2939444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and labels (y)\n",
    "X = questions_tags_df['Cleaned_Text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225bf9a6-b621-4473-9703-4f1df62dd0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a55c3-e69d-4c31-bbb1-32f30a527f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, tokenizer=lambda x: x.split(), ngram_range=(1, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8022267a-3317-4150-96d2-98c945f530e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform on training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594276ca-901e-425d-a7fb-d9094e46e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform validation data\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3887bf-3850-419e-a3cf-ea17469ca3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data and vectorizer\n",
    "questions_tags_df.to_csv('processed_data.csv', index=False)\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "joblib.dump(mlb, 'label_binarizer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1acb73e-3ef4-4711-b0af-06590986e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample model training code\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, hamming_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16191d85-b0f2-49d3-a6ab-298e016d0470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "clf = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "clf.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d116ffe6-8ee1-41d4-b02d-246fb7c28dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "y_pred = clf.predict(X_val_tfidf)\n",
    "print(\"Accuracy: \", accuracy_score(y_val, y_pred))\n",
    "print(\"F1 Score: \", f1_score(y_val, y_pred, average='weighted'))\n",
    "print(\"Hamming Loss: \", hamming_loss(y_val, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
